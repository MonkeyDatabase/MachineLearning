## 背景

1. 在e_greedy算法中，我们通过随机数来决定使用explore还是exploit，完全没有考虑在当前的知识积累下到底是explore好还是exploit好

2. Softmax在机器学习中有非常广泛的应用
   * max的问题
      * `max(1,0.1) = 1`，可以看出当取max时，因为最大的是1，所以结果只可能是1
      * 通过max取数时，只会取到最大的那个数值，造成较小的值`饥饿`，在机器学习中往往是想分值大的一项更有可能取到，而分支小的也不一定不会取到
   * softmax的算法
      * 定义：假设有一个数组V，Vi表示V中的第i个元素，那么**该元素**的Softmax值计算方法为
        $$
        S_{k} = \frac{e^{V_{k}}}{\sum_{i=1}^Ne^{V_{i}}}
        $$
      
   
3. Softmax在增强学习中的应用

   * 计算公式
     $$
     P(k) = \frac{e^{\frac{Q(k)}{τ}}}{\sum_{i=1}^{K}e^{\frac{Q(k)}{τ}}}
     $$

   * 对于同样的τ，P(k)与Q(k)成正比，即exploit越多

   * 对于同样的Q，τ越大，Q(k)之间的差距会被缩小，P(k)会更接近，即explore越多

## 实验

1. softmax数值变化(t = 1.01)
   
   | q_list                       | p_list                                    | comment                |
   | ---------------------------- | ----------------------------------------- | ---------------------- |
   | [0,0]                        | [0.5,0.5]                                 | 平均收益相等，概率相等 |
   | [0.42857142857142855, 0.125] | [0.5745808183729907, 0.42541918162700926] |                        |
   | [0.3333333333333333, 0.125]  | [0.551385591489337, 0.448614408510663]    |                        |
   
2. 实验数据

   ![softmax_different_tau_v1](.HandNote_images\softmax_different_tau_v1.png)

## 局限

* 对于**离散**状态空间、**离散**动作空间上的**多步**强化学习任务，可以看作K-摇臂机问题

  > e_greedy算法、softmax算法对于K-摇臂机问题较为适用，如将强化学习的累计奖赏看作K-摇臂机的累计奖赏、将强化学习的动作看作摇臂，对各个状态均进行一次算法计算统计选择各个摇臂的平均奖赏、选取次数

* 对于连续动作空间等马尔可夫决策过程，这两种摇臂机算法就不太适用了，因为它没有考虑动作对状态的影响

  * 马尔可夫性质：当一个随机过程在给定状态及过去所有状态的情况下，其未来状态的概率分布仅依赖于当前状态。如下列公式表示，与之前的所有状态无关，仅与当前状态有关，在概率论中通常该随机过程具有马尔可夫性质
    $$
    Pr[X(t+h) = y | X(s) = x(s) s\le t] = Pr[X(t+h) = y | X(t) = x(t)] \quad  \forall h \ge t
    $$
    

