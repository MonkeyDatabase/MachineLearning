## 背景

* 模型已知：指定任务对应的马尔可夫决策过程四元组E=<X, A, P, R>均为已知，即机器已对环境进行了建模，能在机器内部模拟出与环境相似或相近的状况

  * X：状态空间

  * A：动作空间

  * P：转移概率
    $$
    P^{a}_{x\rightarrow x^{'}} \quad指在x状态下执行动作a转移到x^{'}的概率
    $$

  * R：转移奖赏
    $$
    R^{a}_{x\rightarrow x^{'}} \quad 指在x状态下执行动作a转移到x^{’}带来的奖赏
    $$

* 策略：在各个状态下进行决策的逻辑

  * 不变：策略是静态的，不随状态的变化而变化
  * 变化：在不同状态下根据策略做出的行为可能发生变化

* 策略评估：评估采用这个策略的收益

  * 状态值函数：
    $$
    V^{\pi}(x)\quad 指从状态x出发使用策略\pi进行决策带来的累积奖赏
    $$

    * T步累积奖赏
      $$
      V_T^\pi(x) = \mathbb{E}_\pi[\frac{1}{T}\sum_{t=1}^Tr_t|x_0=x]
      $$

      > $$
      > \mathbb{E}_{·\sim D}[f(·)] 指f(·)在·服从D分布时的数学期望
      > $$

    * γ折扣累积奖赏
      $$
      V_\gamma^\pi(x)=\mathbb{E}_\pi[\sum_{t=0}^{+\infty}\gamma^tr_{t+1}|x_0=x]
      $$
    
  * 状态-动作值函数：
    $$
    Q^{\pi}(x,a)\quad 指从状态x出发执行动作a后再使用策略\pi带来的累积奖赏
    $$
    
    * T步累积奖赏
      $$
      Q_T^\pi(x,a)=\mathbb{E}_\pi[\frac{1}{T}\sum_{t=1}^{T}r_{t}|x_0=x,a_0=a]
      $$
    
    * γ折扣累积奖赏
      $$
      Q_\gamma^\pi(x,a)=\mathbb{E}_\pi[\sum_{t=0}^{+\infty}\gamma^tr_{t+1}|x_0=x,a_0=a]
      $$

## 独立思考

* 

## 我的疑问

* 状态-动作值函数有什么作用，为什么要先执行一个动作之后才采用策略进行之后决策？当前要选的这个动作有什么独到之处吗？又或者说这个动作至关重要，那状态值函数又有什么价值呢？这两个函数的适用场景有什么差异？
* 状态值函数T步累积奖赏为什么要对T步求和求平均之后再求期望呢？期望不是也是求了一次平均值嘛，为什么取两次平均？我认为里面的那一次不太必要
* 为什么T步累积奖赏的sum是从t=1开始，而γ折扣累积奖赏从t=0开始累加？
* 为什么T步累积奖赏要除以T，不应该是把T步的加起来就好了嘛，然后求这些和的期望？

